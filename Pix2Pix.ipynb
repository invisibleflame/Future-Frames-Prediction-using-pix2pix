{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pix2Pix",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHFvkpNYV8an",
        "outputId": "629b8ef0-9480-47b2-ad56-751fe45ab170"
      },
      "source": [
        "import numpy as np\n",
        "t=[]\n",
        "for i in range(0,17872):\n",
        "    try:\n",
        "        w=np.load(f\"content/Kiti-Single/{i}.npy\")\n",
        "    except:\n",
        "        t.append(i)\n",
        "print(t) #if output is empty then everything is fine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjga8wMGxkbO",
        "outputId": "3e5cfd67-cbed-4a7e-cb63-58786df95f67"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv3D, Conv3DTranspose, BatchNormalization, LeakyReLU, Dropout, ReLU, Input, Concatenate, ZeroPadding3D\n",
        "from tensorflow.keras import Model, Sequential\n",
        "\n",
        "output_channel = 3\n",
        "\n",
        "def downsample(filters,size,shape,apply_batchnorm = True):\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    result = Sequential()\n",
        "\n",
        "    result.add(Conv3D(filters, size, strides=(1,2,2), padding= 'same', batch_input_shape= shape, kernel_initializer= initializer, use_bias= False))\n",
        "\n",
        "    if apply_batchnorm:\n",
        "        result.add(BatchNormalization())\n",
        "\n",
        "    result.add(LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, shape, apply_dropout = False):\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0.,0.02)\n",
        "\n",
        "    result = Sequential()\n",
        "\n",
        "    result.add(Conv3DTranspose(filters,size,strides=(1,2,2),padding='same',batch_input_shape=shape,kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    result.add(BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "\n",
        "        result.add(Dropout(0.5))\n",
        "\n",
        "    result.add(ReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def build_generator():\n",
        "\n",
        "    inputs = Input(shape=[7,256,256,3])\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64,(1,4,4),(None,7,256,256,3),apply_batchnorm=False),\n",
        "        downsample(128,(1,4,4), (None,7, 128,128,64)),\n",
        "        downsample(256, (1,4,4), (None,7,64,64,128)),\n",
        "        downsample(512,(1,4,4),(None,7,32,32,256)),\n",
        "        downsample(512,(1,4,4),(None,7,16,16,512)),\n",
        "        downsample(512,(1,4,4),(None,7,8,8,512)),\n",
        "        downsample(512,(1,4,4),(None,7,4,4,512)),\n",
        "        downsample(512,(1,4,4),(None,7,2,2,512))\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512,(1,4,4),(None,7,1,1,512),apply_dropout=True),\n",
        "        upsample(512,(1,4,4),(None,7,2,2,1024),apply_dropout=True),\n",
        "        upsample(512,(1,4,4),(None,7,4,4,1024),apply_dropout=True),\n",
        "        upsample(512,(1,4,4),(None,7,8,8,1024)),\n",
        "        upsample(256,(1,4,4),(None,7,16,16,1024)),\n",
        "        upsample(128,(1,4,4),(None,7,32,32,512)),\n",
        "        upsample(64,(1,4,4),(None,7,64,64,256))\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0.,0.02)\n",
        "\n",
        "    last = Conv3DTranspose(output_channel,(1,4,4),strides=(1,2,2),padding='same',kernel_initializer=initializer,activation='tanh')\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    skips = []\n",
        "\n",
        "    for down in down_stack:\n",
        "\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip, in zip(up_stack,skips):\n",
        "\n",
        "        x= up(x)\n",
        "        x = Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return Model(inputs = inputs, outputs = x)\n",
        "\n",
        "generator = build_generator()\n",
        "generator.summary()\n",
        "\n",
        "def downs(filters,size,apply_batch_norm = True):\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0.,0.02)\n",
        "\n",
        "    result = Sequential()\n",
        "\n",
        "    result.add(Conv3D(filters, size, strides=2,padding='same',kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_batch_norm:\n",
        "\n",
        "        result.add(BatchNormalization())\n",
        "\n",
        "    result.add(LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def build_discriminator():\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0.,0.02)\n",
        "\n",
        "    inp = Input(shape=[7,256,256,3],name='input_img')\n",
        "    tar = Input(shape=[7,256,256,3],name='target_img')\n",
        "\n",
        "    x = Concatenate()([inp, tar])\n",
        "\n",
        "    down1 = downs(64,(1,4,4),False)(x)\n",
        "    down2 = downs(128,(1,4,4))(down1)\n",
        "    down3 = downs(256,(1,4,4))(down2)\n",
        "\n",
        "    zero_pad1 = ZeroPadding3D()(down3)\n",
        "\n",
        "    conv = Conv3D(512,(1,4,4),strides=1,kernel_initializer=initializer,use_bias=False)(zero_pad1)\n",
        "\n",
        "    batchnorm1 = BatchNormalization()(conv)\n",
        "\n",
        "    leaky_relu = LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = ZeroPadding3D()(leaky_relu)\n",
        "\n",
        "    last = Conv3D(1,(1,4,4),strides=1,kernel_initializer=initializer)(zero_pad2)\n",
        "\n",
        "    return Model(inputs = [inp,tar], outputs = last)\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 7, 256, 256, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 7, 128, 128,  3072        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 7, 64, 64, 12 131584      sequential[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 7, 32, 32, 25 525312      sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 7, 16, 16, 51 2099200     sequential_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_4 (Sequential)       (None, 7, 8, 8, 512) 4196352     sequential_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       (None, 7, 4, 4, 512) 4196352     sequential_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_6 (Sequential)       (None, 7, 2, 2, 512) 4196352     sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_7 (Sequential)       (None, 7, 1, 1, 512) 4196352     sequential_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_8 (Sequential)       (None, 7, 2, 2, 512) 4196352     sequential_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 7, 2, 2, 1024 0           sequential_8[0][0]               \n",
            "                                                                 sequential_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_9 (Sequential)       (None, 7, 4, 4, 512) 8390656     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 7, 4, 4, 1024 0           sequential_9[0][0]               \n",
            "                                                                 sequential_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_10 (Sequential)      (None, 7, 8, 8, 512) 8390656     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 7, 8, 8, 1024 0           sequential_10[0][0]              \n",
            "                                                                 sequential_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_11 (Sequential)      (None, 7, 16, 16, 51 8390656     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 7, 16, 16, 10 0           sequential_11[0][0]              \n",
            "                                                                 sequential_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_12 (Sequential)      (None, 7, 32, 32, 25 4195328     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 7, 32, 32, 51 0           sequential_12[0][0]              \n",
            "                                                                 sequential_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_13 (Sequential)      (None, 7, 64, 64, 12 1049088     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 7, 64, 64, 25 0           sequential_13[0][0]              \n",
            "                                                                 sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_14 (Sequential)      (None, 7, 128, 128,  262400      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 7, 128, 128,  0           sequential_14[0][0]              \n",
            "                                                                 sequential[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_transpose_7 (Conv3DTrans (None, 7, 256, 256,  6147        concatenate_6[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 54,425,859\n",
            "Trainable params: 54,414,979\n",
            "Non-trainable params: 10,880\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_img (InputLayer)          [(None, 7, 256, 256, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "target_img (InputLayer)         [(None, 7, 256, 256, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 7, 256, 256,  0           input_img[0][0]                  \n",
            "                                                                 target_img[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "sequential_15 (Sequential)      (None, 4, 128, 128,  6144        concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_16 (Sequential)      (None, 2, 64, 64, 12 131584      sequential_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_17 (Sequential)      (None, 1, 32, 32, 25 525312      sequential_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding3d (ZeroPadding3D)  (None, 3, 34, 34, 25 0           sequential_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, 3, 31, 31, 51 2097152     zero_padding3d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 3, 31, 31, 51 2048        conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 3, 31, 31, 51 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding3d_1 (ZeroPadding3D (None, 5, 33, 33, 51 0           leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 5, 30, 30, 1) 8193        zero_padding3d_1[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 2,770,433\n",
            "Trainable params: 2,768,641\n",
            "Non-trainable params: 1,792\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48b6f6irFwQr"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "path = 'content/Kiti-Single'\n",
        "\n",
        "LAMBDA = 100\n",
        "epochs = 100\n",
        "\n",
        "def gen_loss(disc_generated_output,gen_output,target):\n",
        "\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    loss = tf.abs(target - gen_output)\n",
        "    l1_loss = tf.reduce_mean(loss)\n",
        "    total_gen_loss = gan_loss + (LAMBDA*l1_loss)\n",
        "\n",
        "    return total_gen_loss,gan_loss,l1_loss\n",
        "\n",
        "def  discriminator_loss(disc_real_output,disc_generated_output):\n",
        "\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    toal_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return toal_disc_loss\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-6,beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-6, beta_1=0.5)\n",
        "\n",
        "checkpoint_dirs = '/content/drive/MyDrive/Pix2PixforFuture'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dirs,\"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer, discriminator_optimizer = discriminator_optimizer, generator = generator, discriminator = discriminator)\n",
        "\n",
        "def generated_img(model,test_input,tar):\n",
        "\n",
        "    prediction = model(test_input,training = True)\n",
        "    plt.figure(figsize=(15,15))\n",
        "\n",
        "    display_list = [test_input[0], tar[0], prediction[0]]\n",
        "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "    for i in range(3):\n",
        "\n",
        "        plt.subplot(1,3,i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i]*0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "log_dir = \"/content/drive/MyDrive/Pix2PixforFuture/logs\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(log_dir + 'fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxOt0dnpyvZW",
        "outputId": "597fa5ec-a20a-41b2-a8d0-7a4596c261bd"
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def train_step(input_img, target, epoch):\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "        gen_output = generator(input_img, training = True)\n",
        "\n",
        "        disc_real_output = discriminator([input_img,target], training = True)\n",
        "        disc_generated_output = discriminator([input_img,gen_output], training = True)\n",
        "\n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = gen_loss(disc_generated_output,gen_output,target)\n",
        "\n",
        "        disc_loss = discriminator_loss(disc_real_output,disc_generated_output)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(gen_total_loss,generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients,generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients((zip(discriminator_gradients,discriminator.trainable_variables)))\n",
        "\n",
        "    with summary_writer.as_default():\n",
        "\n",
        "        tf.summary.scalar('gen_total_loss',gen_total_loss, epoch)\n",
        "        tf.summary.scalar('gen_gan_loss', gen_gan_loss,step=epoch)\n",
        "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "        tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
        "\n",
        "def fit(epochs):\n",
        "\n",
        "    batch_size = 4\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        print(\"Epoch {}\".format(epoch))\n",
        "        for _,_,files in os.walk(path):\n",
        "          random.shuffle(files)\n",
        "          total_input = np.zeros((batch_size,7,256,256,3))\n",
        "          total_output = np.zeros((batch_size,7,256,256,3))\n",
        "          num = 0\n",
        "      \n",
        "          for k,file in enumerate(files):\n",
        "              m = int(file.split('.')[0])\n",
        "              # print(m)\n",
        "              if m <17850: \n",
        "                # print(\"Loopy\")\n",
        "                \n",
        "                xm = np.load(path + \"/{}.npy\".format(m))\n",
        "                xm_plus1 = np.load(path + \"/{}.npy\".format(m+1))\n",
        "                xm_plus2 = np.load(path + \"/{}.npy\".format(m+2))\n",
        "                xm_plus3 = np.load(path + \"/{}.npy\".format(m+3))\n",
        "                xm_plus4 = np.load(path + \"/{}.npy\".format(m+4))\n",
        "                xm_plus5 = np.load(path + \"/{}.npy\".format(m+5))\n",
        "                xm_plus6 = np.load(path + \"/{}.npy\".format(m+6))\n",
        "                xm_plus7 = np.load(path + \"/{}.npy\".format(m+7))\n",
        "                xm_plus8 = np.load(path + \"/{}.npy\".format(m+8))\n",
        "                xm_plus9 = np.load(path + \"/{}.npy\".format(m+9))\n",
        "                xm_plus10 = np.load(path + \"/{}.npy\".format(m+10))\n",
        "                xm_plus11 = np.load(path + \"/{}.npy\".format(m+11))\n",
        "                xm_plus12 = np.load(path + \"/{}.npy\".format(m+12))\n",
        "                xm_plus13 = np.load(path + \"/{}.npy\".format(m+13))\n",
        "\n",
        "                input = np.reshape(np.concatenate([xm,xm_plus1,xm_plus2,xm_plus3,xm_plus4,xm_plus5,xm_plus6]),(7,256,256,3)).astype(np.float32)\n",
        "                output = np.reshape(np.concatenate([xm_plus7,xm_plus8,xm_plus9,xm_plus10,xm_plus11,xm_plus12,xm_plus13]),(7,256,256,3)).astype(np.float32)\n",
        "\n",
        "                total_input[num] = input/255.0\n",
        "                total_output[num] = output/255.0\n",
        "\n",
        "                num = num + 1\n",
        "\n",
        "                if k%1000==0: \n",
        "                    print(k)\n",
        "                if num == batch_size:\n",
        "                  num = 0\n",
        "                  total_input = tf.convert_to_tensor(total_input,dtype=tf.float32)\n",
        "                  total_output = tf.convert_to_tensor(total_output,dtype=tf.float32)\n",
        "                  train_step(total_input, total_output, epoch)\n",
        "                  total_input = np.zeros((batch_size,7,256,256,3))\n",
        "                  total_output = np.zeros((batch_size,7,256,256,3))\n",
        "        print(\"Time taken for epoch {} is {} sec \\n\".format(epoch+1, time.time()-start))\n",
        "\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "print(\"Started Restoring\")\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dirs))\n",
        "print(\"Done Restoring\")\n",
        "fit(epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started Restoring\n",
            "Done Restoring\n",
            "Epoch 0\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "Time taken for epoch 1 is 3362.5772783756256 sec \n",
            "\n",
            "Epoch 1\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "Time taken for epoch 2 is 2915.188324689865 sec \n",
            "\n",
            "Epoch 2\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "Time taken for epoch 3 is 2913.2279963493347 sec \n",
            "\n",
            "Epoch 3\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "Time taken for epoch 4 is 2917.6775209903717 sec \n",
            "\n",
            "Epoch 4\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "Time taken for epoch 5 is 2914.7374143600464 sec \n",
            "\n",
            "Epoch 5\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "Time taken for epoch 6 is 2912.5883407592773 sec \n",
            "\n",
            "Epoch 6\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "Time taken for epoch 7 is 2914.23011469841 sec \n",
            "\n",
            "Epoch 7\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3pGntki4C-b"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dirs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsubGBC0ZD1D"
      },
      "source": [
        "m = 12230\n",
        "xm = np.load(path + \"/{}.npy\".format(m))\n",
        "xm_plus1 = np.load(path + \"/{}.npy\".format(m+1))\n",
        "xm_plus2 = np.load(path + \"/{}.npy\".format(m+2))\n",
        "xm_plus3 = np.load(path + \"/{}.npy\".format(m+3))\n",
        "xm_plus4 = np.load(path + \"/{}.npy\".format(m+4))\n",
        "xm_plus5 = np.load(path + \"/{}.npy\".format(m+5))\n",
        "xm_plus6 = np.load(path + \"/{}.npy\".format(m+6))\n",
        "\n",
        "input = np.reshape(np.concatenate([xm,xm_plus1,xm_plus2,xm_plus3,xm_plus4,xm_plus5,xm_plus6]),(7,256,256,3)).astype(np.float32)\n",
        "\n",
        "total_input = tf.expand_dims(input, axis = 0)\n",
        "\n",
        "gen_output = generator(total_input/255.0, training = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAvF3IqUN3oI",
        "outputId": "3a49d8d4-7d51-456a-b78c-3ded8897908d"
      },
      "source": [
        "print(gen_output[0][1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33DTSE5GZ2Ov"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "#gen_output = tf.squeeze(gen_output, axis=0)\n",
        "\n",
        "for i in range(7):\n",
        "  cv2_imshow(input[i])\n",
        "\n",
        "print(\"GO\")\n",
        "\n",
        "for i in range(7):\n",
        "  cv2_imshow(gen_output[0][i].numpy()*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re5JxG3heQNH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}